{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gensim library\n",
    "#Loading gensim\n",
    "\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a corpus from a list of text\n",
    "common_dictionary = Dictionary(common_texts)\n",
    "\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "lda = LdaModel(common_corpus, num_topics=10)\n",
    "\n",
    "#new corpus of unseen documents\n",
    "other_texts = [\n",
    "    ['data', 'unstructured', 'time'],\n",
    "    ['bigdata', 'intelligence', 'natural'],\n",
    "    ['language', 'machine', 'computer']\n",
    "]\n",
    "other_corpus = [common_dictionary.doc2bow(text) for text in other_texts]\n",
    "unseen_doc = other_corpus[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.050000045), (1, 0.05000921), (2, 0.05000004), (3, 0.050000045), (4, 0.05000004), (5, 0.050000045), (6, 0.050000045), (7, 0.5499905), (8, 0.05000004), (9, 0.05000004)]\n"
     ]
    }
   ],
   "source": [
    "#get topic probability distribution for a document\n",
    "vector = lda[unseen_doc]\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify topics from news"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Assisted Practice: Identify Topics from News Items\n",
    "In this demo, we will show you how to implement the concept of \"Topic Modeling\" using Gensim.\n",
    "LDA is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions.\n",
    "Each document is modeled as a multinomial distribution of topics and each topic is modeled as a multinomial distribution of words.\n",
    "LDA assumes that every chunk of text we feed into it will contain words that are somehow related. Therefore, choosing the right corpus of data is crucial.\n",
    "It also assumes that documents are produced from a mixture of topics. Those topics then generate words based on their probability of distribution.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset=>\n",
    "\n",
    "The dataset we'll use is the 20newsgroup dataset that is available from sklearn. \n",
    "This dataset has news articles grouped into 20 news categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle = True)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(list(newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### there are different themes here indicating news in sports, religion, politics, tech etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing\n",
    "\n",
    "We will perform the following steps:\n",
    "\n",
    "● Tokenization: Split the text into sentences and sentences into words. Lowercase the words and remove punctuations.\n",
    "\n",
    "● Words that have fewer than 3 characters are removed.\n",
    "\n",
    "● All stopwords are removed.\n",
    "\n",
    "● Words are lemmatized in such a way that words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
    "\n",
    "● Words are stemmed in such a way that words are reduced to their root form.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\",\n",
       " \"From: guykuo@carson.u.washington.edu (Guy Kuo)\\nSubject: SI Clock Poll - Final Call\\nSummary: Final call for SI clock reports\\nKeywords: SI,acceleration,clock,upgrade\\nArticle-I.D.: shelley.1qvfo9INNc3s\\nOrganization: University of Washington\\nLines: 11\\nNNTP-Posting-Host: carson.u.washington.edu\\n\\nA fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\\n\\nGuy Kuo <guykuo@u.washington.edu>\\n\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample news\n",
    "newsgroups_train.data[:2]\n",
    "# this needs cleaning, removal of stop words, stemming, lemmatization, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing\n",
    "\n",
    "We will perform the following steps:\n",
    "\n",
    "●\tTokenization: Split the text into sentences and sentences into words. Lowercase the words and remove punctuations.\n",
    "\n",
    "●\tWords that have fewer than 3 characters are removed.\n",
    "\n",
    "●\tAll stopwords are removed.\n",
    "\n",
    "●\tWords are lemmatized in such a way that words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
    "\n",
    "●\tWords are stemmed in such a way that words are reduced to their root form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/labsuser/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "#trying to see if lemmatizer works\n",
    "print(WordNetLemmatizer().lemmatize('went', pos = 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caress</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mules</td>\n",
       "      <td>mule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siezing</td>\n",
       "      <td>siez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reference</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colonizer</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>plotted</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed\n",
       "0         caress  caress\n",
       "1          flies     fli\n",
       "2           dies     die\n",
       "3          mules    mule\n",
       "4         denied    deni\n",
       "5           died     die\n",
       "6         agreed    agre\n",
       "7          owned     own\n",
       "8        humbled   humbl\n",
       "9          sized    size\n",
       "10       meeting    meet\n",
       "11       stating   state\n",
       "12       siezing    siez\n",
       "13   itemization    item\n",
       "14   sensational  sensat\n",
       "15   traditional  tradit\n",
       "16     reference   refer\n",
       "17     colonizer   colon\n",
       "18       plotted    plot"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see if dtrmming works\n",
    "import pandas as pd\n",
    "stemmer = SnowballStemmer('english')\n",
    "original_words = ['caress','flies','dies','mules',  'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "\n",
    "pd.DataFrame(data={'original word':original_words, 'stemmed':singles })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply lem and stemming on the whole dataset\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      "['This', 'disk', 'has', 'failed', 'many', 'times.', 'I', 'would', 'like', 'to', 'get', 'it', 'replaced.']\n",
      "\n",
      "\n",
      "Tokenized and lemmatized document: \n",
      "['disk', 'fail', 'time', 'like', 'replac']\n"
     ]
    }
   ],
   "source": [
    "#preview the doc after preprocessing\n",
    "document_num = 50 #id for the doc\n",
    "doc_sample = 'This disk has failed many times. I would like to get it replaced.'\n",
    "\n",
    "print(\"Original document: \")\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print(\"\\n\\nTokenized and lemmatized document: \")\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['lerxst', 'thing', 'subject', 'nntp', 'post', 'host', 'organ', 'univers', 'maryland', 'colleg', 'park', 'line', 'wonder', 'enlighten', 'door', 'sport', 'look', 'late', 'earli', 'call', 'bricklin', 'door', 'small', 'addit', 'bumper', 'separ', 'rest', 'bodi', 'know', 'tellm', 'model', 'engin', 'spec', 'year', 'product', 'histori', 'info', 'funki', 'look', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst'], ['guykuo', 'carson', 'washington', 'subject', 'clock', 'poll', 'final', 'summari', 'final', 'clock', 'report', 'keyword', 'acceler', 'clock', 'upgrad', 'articl', 'shelley', 'qvfo', 'innc', 'organ', 'univers', 'washington', 'line', 'nntp', 'post', 'host', 'carson', 'washington', 'fair', 'number', 'brave', 'soul', 'upgrad', 'clock', 'oscil', 'share', 'experi', 'poll', 'send', 'brief', 'messag', 'detail', 'experi', 'procedur', 'speed', 'attain', 'rat', 'speed', 'card', 'adapt', 'heat', 'sink', 'hour', 'usag', 'floppi', 'disk', 'function', 'floppi', 'especi', 'request', 'summar', 'day', 'network', 'knowledg', 'base', 'clock', 'upgrad', 'haven', 'answer', 'poll', 'thank', 'guykuo', 'washington']]\n"
     ]
    }
   ],
   "source": [
    "#now we preprocess all the news headline we have, we iterate the list of docs in our training sample \n",
    "processed_docs = []\n",
    "\n",
    "for doc in newsgroups_train.data:\n",
    "    processed_docs.append(preprocess(doc))\n",
    "\n",
    "print(processed_docs[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOW in the dataset\n",
    "\n",
    "#we do  this here by creating a dict from processed_docs\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 addit\n",
      "1 bodi\n",
      "2 bricklin\n",
      "3 bring\n",
      "4 bumper\n",
      "5 call\n",
      "6 colleg\n",
      "7 door\n",
      "8 earli\n",
      "9 engin\n",
      "10 enlighten\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim filter_extremes\n",
    "filter out tokrns that appear in:\n",
    "1. less than no_below docs(absolute number) or\n",
    "2. more than no_above docs(fraction of total corpus size, not abs number)\n",
    "3. after (1) and (2) keep only the first keep_n most freq tokens(or keep all if None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim doc2bow\n",
    "\n",
    "\n",
    "Convert document (a list of words) into the bag-of-words format = list of (token_id, token_count) 2-tuples. Each word is assumed to be a tokenized and normalized string (either unicode or utf8-encoded). No further preprocessing is done on the words in the document; apply tokenization, stemming, etc. before calling this method.\n",
    "\n",
    "● Create the bag-of-words model for each document i.e for each document we create a dictionary reporting how many words and how many times those words appear. Save this to 'bow_corpus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 18 (\"rest\") appears 1 time.\n",
      "Word 166 (\"clear\") appears 1 time.\n",
      "Word 336 (\"refer\") appears 1 time.\n",
      "Word 350 (\"true\") appears 1 time.\n",
      "Word 391 (\"technolog\") appears 1 time.\n",
      "Word 437 (\"christian\") appears 1 time.\n",
      "Word 453 (\"exampl\") appears 1 time.\n",
      "Word 476 (\"jew\") appears 1 time.\n",
      "Word 480 (\"lead\") appears 1 time.\n",
      "Word 482 (\"littl\") appears 3 time.\n",
      "Word 520 (\"wors\") appears 2 time.\n",
      "Word 721 (\"keith\") appears 3 time.\n",
      "Word 732 (\"punish\") appears 1 time.\n",
      "Word 803 (\"california\") appears 1 time.\n",
      "Word 859 (\"institut\") appears 1 time.\n",
      "Word 917 (\"similar\") appears 1 time.\n",
      "Word 990 (\"allan\") appears 1 time.\n",
      "Word 991 (\"anti\") appears 1 time.\n",
      "Word 992 (\"arriv\") appears 1 time.\n",
      "Word 993 (\"austria\") appears 1 time.\n",
      "Word 994 (\"caltech\") appears 2 time.\n",
      "Word 995 (\"distinguish\") appears 1 time.\n",
      "Word 996 (\"german\") appears 1 time.\n",
      "Word 997 (\"germani\") appears 3 time.\n",
      "Word 998 (\"hitler\") appears 1 time.\n",
      "Word 999 (\"livesey\") appears 2 time.\n",
      "Word 1000 (\"motto\") appears 2 time.\n",
      "Word 1001 (\"order\") appears 1 time.\n",
      "Word 1002 (\"pasadena\") appears 1 time.\n",
      "Word 1003 (\"pompous\") appears 1 time.\n",
      "Word 1004 (\"popul\") appears 1 time.\n",
      "Word 1005 (\"rank\") appears 1 time.\n",
      "Word 1006 (\"schneider\") appears 1 time.\n",
      "Word 1007 (\"semit\") appears 1 time.\n",
      "Word 1008 (\"social\") appears 1 time.\n",
      "Word 1009 (\"solntz\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "document_num = 20\n",
    "bow_doc_x = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_x)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
    "                                                     dictionary[bow_doc_x[i][0]], \n",
    "                                                     bow_doc_x[i][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train LDA model using gensim models LdaMulticore and save to lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model =gensim.models.LdaMulticore(bow_corpus,\n",
    "                                       num_topics=8,\n",
    "                                       id2word= dictionary,\n",
    "                                       passes=10, #like epochs\n",
    "                                       workers = 2) #like 2 parallel processing ofdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each topic we explore the words that occur in that topic and its relative weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.008*\"bike\" + 0.005*\"game\" + 0.005*\"team\" + 0.004*\"run\" + 0.004*\"virginia\" + 0.004*\"player\" + 0.004*\"play\" + 0.004*\"homosexu\" + 0.003*\"pitch\" + 0.003*\"motorcycl\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.009*\"govern\" + 0.007*\"armenian\" + 0.006*\"israel\" + 0.005*\"kill\" + 0.005*\"isra\" + 0.004*\"american\" + 0.004*\"turkish\" + 0.004*\"countri\" + 0.004*\"weapon\" + 0.004*\"live\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.016*\"game\" + 0.014*\"team\" + 0.011*\"play\" + 0.008*\"hockey\" + 0.008*\"player\" + 0.005*\"season\" + 0.005*\"canada\" + 0.004*\"leagu\" + 0.004*\"score\" + 0.004*\"toronto\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.010*\"card\" + 0.010*\"window\" + 0.008*\"driver\" + 0.007*\"sale\" + 0.006*\"price\" + 0.005*\"speed\" + 0.005*\"appl\" + 0.005*\"monitor\" + 0.005*\"video\" + 0.005*\"drive\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.014*\"file\" + 0.010*\"program\" + 0.009*\"window\" + 0.006*\"encrypt\" + 0.006*\"chip\" + 0.006*\"data\" + 0.006*\"imag\" + 0.006*\"avail\" + 0.005*\"version\" + 0.004*\"code\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.012*\"space\" + 0.009*\"nasa\" + 0.006*\"scienc\" + 0.005*\"orbit\" + 0.004*\"research\" + 0.004*\"launch\" + 0.004*\"pitt\" + 0.003*\"earth\" + 0.003*\"develop\" + 0.003*\"bank\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.032*\"drive\" + 0.015*\"scsi\" + 0.011*\"disk\" + 0.009*\"hard\" + 0.008*\"control\" + 0.007*\"columbia\" + 0.007*\"washington\" + 0.006*\"uiuc\" + 0.004*\"jpeg\" + 0.004*\"car\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.012*\"christian\" + 0.008*\"jesus\" + 0.006*\"exist\" + 0.005*\"word\" + 0.005*\"moral\" + 0.005*\"bibl\" + 0.005*\"religion\" + 0.004*\"life\" + 0.004*\"church\" + 0.004*\"claim\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "## Word Analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we will show you how to train and evaluate Word2Vec models in your business data using NLP.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import gensim \n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = open(\"word_analogy.txt\", \"r\", encoding='cp1252') \n",
    "s = sample.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = s.replace(\"\\n\", \" \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through each sentence in the file \n",
    "for i in sent_tokenize(f): \n",
    "    temp = [] \n",
    "      \n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(i): \n",
    "        temp.append(j.lower()) \n",
    "  \n",
    "    data.append(temp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create CBOW Model and print results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              size = 100, window = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'wonderland' - CBOW :  0.9994715\n",
      "Cosine similarity between 'alice' and 'machines' - CBOW :  0.9927303\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity between 'alice' \" + \n",
    "               \"and 'wonderland' - CBOW : \", \n",
    "    model1.similarity('alice', 'wonderland')) \n",
    "      \n",
    "print(\"Cosine similarity between 'alice' \" +\n",
    "                 \"and 'machines' - CBOW : \", \n",
    "      model1.similarity('alice', 'machines')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Skip Gram Model and print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100, \n",
    "                                             window = 5, sg = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'wonderland' - Skip Gram :  0.89384156\n",
      "Cosine similarity between 'alice' and 'machines' - Skip Gram :  0.8489359\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity between 'alice' \" +\n",
    "          \"and 'wonderland' - Skip Gram : \", \n",
    "    model2.similarity('alice', 'wonderland')) \n",
    "      \n",
    "print(\"Cosine similarity between 'alice' \" +\n",
    "            \"and 'machines' - Skip Gram : \", \n",
    "      model2.similarity('alice', 'machines')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
